{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6119d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        logger.info(f\"Loading CSV: {csv_file}\")\n",
    "        self.pairs_df = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.valid_indices = self._validate_paths()\n",
    "        logger.info(f\"Found {len(self.valid_indices)} valid pairs out of {len(self.pairs_df)}\")\n",
    "    \n",
    "    def _validate_paths(self):\n",
    "        valid_indices = []\n",
    "        for idx in range(len(self.pairs_df)):\n",
    "            id_img_path = self.pairs_df.iloc[idx]['id_image_path']\n",
    "            person_img_path = self.pairs_df.iloc[idx]['person_image_path']\n",
    "            try:\n",
    "                if not os.path.isfile(id_img_path) or not os.path.isfile(person_img_path):\n",
    "                    logger.warning(f\"Invalid paths at index {idx}: {id_img_path}, {person_img_path}\")\n",
    "                    continue\n",
    "                Image.open(id_img_path).convert('RGB').close()\n",
    "                Image.open(person_img_path).convert('RGB').close()\n",
    "                valid_indices.append(idx)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Cannot read images at index {idx}: {id_img_path}, {person_img_path} - {e}\")\n",
    "        return valid_indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        actual_idx = self.valid_indices[idx]\n",
    "        id_img_path = self.pairs_df.iloc[actual_idx]['id_image_path']\n",
    "        person_img_path = self.pairs_df.iloc[actual_idx]['person_image_path']\n",
    "        label = self.pairs_df.iloc[actual_idx]['label']\n",
    "        \n",
    "        try:\n",
    "            id_img = Image.open(id_img_path).convert('RGB')\n",
    "            person_img = Image.open(person_img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading images at index {actual_idx}: {id_img_path}, {person_img_path} - {e}\")\n",
    "            return None\n",
    "        \n",
    "        if self.transform:\n",
    "            try:\n",
    "                id_img = self.transform(id_img)\n",
    "                person_img = self.transform(person_img)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error applying transform at index {actual_idx}: {e}\")\n",
    "                return None\n",
    "        \n",
    "        return id_img, person_img, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        in_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 128)\n",
    "        )\n",
    "    \n",
    "    def forward_one(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_one(input1)\n",
    "        output2 = self.forward_one(input2)\n",
    "        return output1, output2\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = torch.nn.functional.pairwise_distance(output1, output2)\n",
    "        loss_same = label * torch.pow(euclidean_distance, 2)\n",
    "        loss_diff = (1 - label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        loss = torch.mean(loss_same + loss_diff)\n",
    "        return loss, euclidean_distance\n",
    "\n",
    "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, output_dir):\n",
    "    epoch_losses = []\n",
    "    matching_distances = []\n",
    "    non_matching_distances = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        batch_count = 0\n",
    "        total_batches = len(train_loader)\n",
    "        logger.info(f\"Epoch {epoch+1}/{num_epochs}, {total_batches} batches\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            if batch is None:\n",
    "                logger.warning(f\"Skipping invalid batch {batch_idx+1}/{total_batches}\")\n",
    "                continue\n",
    "            \n",
    "            img1, img2, label = batch\n",
    "            img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output1, output2 = model(img1, img2)\n",
    "            loss, _ = criterion(output1, output2, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * img1.size(0)\n",
    "            batch_count += img1.size(0)\n",
    "            \n",
    "            if (batch_idx + 1) % 20 == 0 or (batch_idx + 1) == total_batches:\n",
    "                logger.info(f\"Batch {batch_idx+1}/{total_batches}, Batch Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        if batch_count == 0:\n",
    "            logger.error(\"No valid batches processed in epoch\")\n",
    "            return None, None, None\n",
    "        \n",
    "        epoch_loss = running_loss / batch_count\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        logger.info(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        model.eval()\n",
    "        matching_dist = []\n",
    "        non_matching_dist = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                if batch is None:\n",
    "                    continue\n",
    "                img1, img2, label = batch\n",
    "                img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "                output1, output2 = model(img1, img2)\n",
    "                _, dist = criterion(output1, output2, label)\n",
    "                for d, l in zip(dist.cpu().numpy(), label.cpu().numpy()):\n",
    "                    if l == 1:\n",
    "                        matching_dist.append(d)\n",
    "                    else:\n",
    "                        non_matching_dist.append(d)\n",
    "        \n",
    "        mean_matching = np.mean(matching_dist) if matching_dist else float('inf')\n",
    "        mean_non_matching = np.mean(non_matching_dist) if non_matching_dist else 0.0\n",
    "        gap = mean_non_matching - mean_matching\n",
    "        matching_distances.append(mean_matching)\n",
    "        non_matching_distances.append(mean_non_matching)\n",
    "        logger.info(f\"Validation: Matching Dist: {mean_matching:.4f}, Non-Matching Dist: {mean_non_matching:.4f}, Gap: {gap:.4f}\")\n",
    "        \n",
    "        scheduler.step(epoch_loss)\n",
    "    \n",
    "    model_path = os.path.join(output_dir, 'siamese_model_final.pth')\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    logger.info(f\"Saved final model at {model_path}\")\n",
    "    \n",
    "    epochs = range(1, num_epochs + 1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(epochs, matching_distances, width=0.4, label='Matching Distance', color='blue', alpha=0.5)\n",
    "    plt.bar([e + 0.4 for e in epochs], non_matching_distances, width=0.4, label='Non-Matching Distance', color='red', alpha=0.5)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.title('Matching vs Non-Matching Distances per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plot_path = os.path.join(output_dir, 'distance_plot1.png')\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    logger.info(f\"Saved distance plot at {plot_path}\")\n",
    "    \n",
    "    return epoch_losses, matching_distances, non_matching_distances\n",
    "\n",
    "def main():\n",
    "    train_csv = r\"D:\\Projects\\PhotosWorkl\\train_pairs_balanced.csv\"\n",
    "    test_csv = r\"D:\\Projects\\PhotosWorkl\\test_pairs_balanced.csv\"\n",
    "    output_dir = r\"D:\\Projects\\PhotosWorkl\\\\\"\n",
    "    batch_size = 32\n",
    "    num_epochs = 15\n",
    "    learning_rate = 0.0005\n",
    "    weight_decay = 0.0001\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    \n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    logger.info(\"Loading datasets\")\n",
    "    train_dataset = SiameseDataset(train_csv, transform=train_transform)\n",
    "    test_dataset = SiameseDataset(test_csv, transform=test_transform)\n",
    "    \n",
    "    if len(train_dataset) == 0:\n",
    "        logger.error(\"No valid training pairs. Check CSV paths and image accessibility.\")\n",
    "        return\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  \n",
    "        pin_memory=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Initializing model\")\n",
    "    model = SiameseNetwork().to(device)\n",
    "    criterion = ContrastiveLoss(margin=1.0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "    logger.info(\"Starting training\")\n",
    "    epoch_losses, matching_distances, non_matching_distances = train_and_validate(\n",
    "        model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs, device, output_dir\n",
    "    )\n",
    "    \n",
    "    if epoch_losses is not None:\n",
    "        logger.info(\"Training completed\")\n",
    "    else:\n",
    "        logger.error(\"Training failed. Check logs for details.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "576ce52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-22 16:40:44,173 - INFO - Using device: cuda\n",
      "2025-05-22 16:40:44,174 - INFO - Loading datasets\n",
      "2025-05-22 16:40:44,175 - INFO - Loading CSV: D:\\Projects\\PhotosWorkl\\train_pairs_balanced.csv\n",
      "2025-05-22 16:40:47,593 - INFO - Found 4658 valid pairs out of 4658\n",
      "2025-05-22 16:40:47,594 - INFO - Loading CSV: D:\\Projects\\PhotosWorkl\\test_pairs_balanced.csv\n",
      "2025-05-22 16:40:48,443 - INFO - Found 1166 valid pairs out of 1166\n",
      "2025-05-22 16:40:48,444 - INFO - Initializing model\n",
      "2025-05-22 16:40:48,593 - INFO - Starting training\n",
      "2025-05-22 16:40:48,594 - INFO - Epoch 1/15, 146 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vlad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Vlad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\Vlad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-22 16:41:02,527 - INFO - Batch 20/146, Batch Loss: 0.2318\n",
      "2025-05-22 16:41:13,621 - INFO - Batch 40/146, Batch Loss: 0.2282\n",
      "2025-05-22 16:41:24,960 - INFO - Batch 60/146, Batch Loss: 0.3668\n",
      "2025-05-22 16:41:36,432 - INFO - Batch 80/146, Batch Loss: 0.2189\n",
      "2025-05-22 16:41:47,703 - INFO - Batch 100/146, Batch Loss: 0.2916\n",
      "2025-05-22 16:41:59,415 - INFO - Batch 120/146, Batch Loss: 0.3006\n",
      "2025-05-22 16:42:11,830 - INFO - Batch 140/146, Batch Loss: 0.3113\n",
      "2025-05-22 16:42:15,547 - INFO - Batch 146/146, Batch Loss: 0.2492\n",
      "2025-05-22 16:42:15,548 - INFO - Epoch 1/15, Train Loss: 0.4252\n",
      "2025-05-22 16:42:22,030 - INFO - Validation: Matching Dist: 0.1139, Non-Matching Dist: 0.1164, Gap: 0.0025\n",
      "2025-05-22 16:42:22,032 - INFO - Epoch 2/15, 146 batches\n",
      "2025-05-22 16:42:33,304 - INFO - Batch 20/146, Batch Loss: 0.2365\n",
      "2025-05-22 16:42:44,768 - INFO - Batch 40/146, Batch Loss: 0.2735\n",
      "2025-05-22 16:42:58,524 - INFO - Batch 60/146, Batch Loss: 0.2971\n",
      "2025-05-22 16:43:09,479 - INFO - Batch 80/146, Batch Loss: 0.2535\n",
      "2025-05-22 16:43:20,066 - INFO - Batch 100/146, Batch Loss: 0.2332\n",
      "2025-05-22 16:43:30,764 - INFO - Batch 120/146, Batch Loss: 0.2936\n",
      "2025-05-22 16:43:42,016 - INFO - Batch 140/146, Batch Loss: 0.2791\n",
      "2025-05-22 16:43:45,036 - INFO - Batch 146/146, Batch Loss: 0.2620\n",
      "2025-05-22 16:43:45,037 - INFO - Epoch 2/15, Train Loss: 0.2691\n",
      "2025-05-22 16:43:51,163 - INFO - Validation: Matching Dist: 0.0440, Non-Matching Dist: 0.0454, Gap: 0.0013\n",
      "2025-05-22 16:43:51,165 - INFO - Epoch 3/15, 146 batches\n",
      "2025-05-22 16:44:04,277 - INFO - Batch 20/146, Batch Loss: 0.2640\n",
      "2025-05-22 16:44:18,138 - INFO - Batch 40/146, Batch Loss: 0.2756\n",
      "2025-05-22 16:44:29,885 - INFO - Batch 60/146, Batch Loss: 0.2657\n",
      "2025-05-22 16:44:43,255 - INFO - Batch 80/146, Batch Loss: 0.2498\n",
      "2025-05-22 16:44:54,571 - INFO - Batch 100/146, Batch Loss: 0.2554\n",
      "2025-05-22 16:45:07,514 - INFO - Batch 120/146, Batch Loss: 0.2563\n",
      "2025-05-22 16:45:19,124 - INFO - Batch 140/146, Batch Loss: 0.2526\n",
      "2025-05-22 16:45:23,414 - INFO - Batch 146/146, Batch Loss: 0.2443\n",
      "2025-05-22 16:45:23,415 - INFO - Epoch 3/15, Train Loss: 0.2676\n",
      "2025-05-22 16:45:31,690 - INFO - Validation: Matching Dist: 0.1522, Non-Matching Dist: 0.1569, Gap: 0.0046\n",
      "2025-05-22 16:45:31,691 - INFO - Epoch 4/15, 146 batches\n",
      "2025-05-22 16:45:45,545 - INFO - Batch 20/146, Batch Loss: 0.2020\n",
      "2025-05-22 16:46:00,189 - INFO - Batch 40/146, Batch Loss: 0.2723\n",
      "2025-05-22 16:46:15,291 - INFO - Batch 60/146, Batch Loss: 0.2810\n",
      "2025-05-22 16:46:27,670 - INFO - Batch 80/146, Batch Loss: 0.3057\n",
      "2025-05-22 16:46:39,254 - INFO - Batch 100/146, Batch Loss: 0.2820\n",
      "2025-05-22 16:46:50,482 - INFO - Batch 120/146, Batch Loss: 0.2432\n",
      "2025-05-22 16:47:04,089 - INFO - Batch 140/146, Batch Loss: 0.2708\n",
      "2025-05-22 16:47:07,936 - INFO - Batch 146/146, Batch Loss: 0.2374\n",
      "2025-05-22 16:47:07,937 - INFO - Epoch 4/15, Train Loss: 0.2650\n",
      "2025-05-22 16:47:16,016 - INFO - Validation: Matching Dist: 0.1091, Non-Matching Dist: 0.1178, Gap: 0.0087\n",
      "2025-05-22 16:47:16,019 - INFO - Epoch 5/15, 146 batches\n",
      "2025-05-22 16:47:29,767 - INFO - Batch 20/146, Batch Loss: 0.2542\n",
      "2025-05-22 16:47:43,184 - INFO - Batch 40/146, Batch Loss: 0.2688\n",
      "2025-05-22 16:47:53,779 - INFO - Batch 60/146, Batch Loss: 0.2603\n",
      "2025-05-22 16:48:04,674 - INFO - Batch 80/146, Batch Loss: 0.2439\n",
      "2025-05-22 16:48:17,026 - INFO - Batch 100/146, Batch Loss: 0.2343\n",
      "2025-05-22 16:48:28,437 - INFO - Batch 120/146, Batch Loss: 0.2973\n",
      "2025-05-22 16:48:42,285 - INFO - Batch 140/146, Batch Loss: 0.2527\n",
      "2025-05-22 16:48:46,248 - INFO - Batch 146/146, Batch Loss: 0.2185\n",
      "2025-05-22 16:48:46,249 - INFO - Epoch 5/15, Train Loss: 0.2613\n",
      "2025-05-22 16:48:54,285 - INFO - Validation: Matching Dist: 0.0741, Non-Matching Dist: 0.0822, Gap: 0.0081\n",
      "2025-05-22 16:48:54,286 - INFO - Epoch 6/15, 146 batches\n",
      "2025-05-22 16:49:07,524 - INFO - Batch 20/146, Batch Loss: 0.2576\n",
      "2025-05-22 16:49:22,059 - INFO - Batch 40/146, Batch Loss: 0.2733\n",
      "2025-05-22 16:49:33,159 - INFO - Batch 60/146, Batch Loss: 0.2442\n",
      "2025-05-22 16:49:46,777 - INFO - Batch 80/146, Batch Loss: 0.2619\n",
      "2025-05-22 16:49:58,434 - INFO - Batch 100/146, Batch Loss: 0.2901\n",
      "2025-05-22 16:50:14,218 - INFO - Batch 120/146, Batch Loss: 0.2816\n",
      "2025-05-22 16:50:26,919 - INFO - Batch 140/146, Batch Loss: 0.2881\n",
      "2025-05-22 16:50:30,422 - INFO - Batch 146/146, Batch Loss: 0.2791\n",
      "2025-05-22 16:50:30,423 - INFO - Epoch 6/15, Train Loss: 0.2599\n",
      "2025-05-22 16:50:37,922 - INFO - Validation: Matching Dist: 0.1018, Non-Matching Dist: 0.1013, Gap: -0.0005\n",
      "2025-05-22 16:50:37,923 - INFO - Epoch 7/15, 146 batches\n",
      "2025-05-22 16:50:52,778 - INFO - Batch 20/146, Batch Loss: 0.2704\n",
      "2025-05-22 16:51:05,173 - INFO - Batch 40/146, Batch Loss: 0.2511\n",
      "2025-05-22 16:51:17,764 - INFO - Batch 60/146, Batch Loss: 0.2945\n",
      "2025-05-22 16:51:31,513 - INFO - Batch 80/146, Batch Loss: 0.2489\n",
      "2025-05-22 16:51:44,869 - INFO - Batch 100/146, Batch Loss: 0.2635\n",
      "2025-05-22 16:51:59,347 - INFO - Batch 120/146, Batch Loss: 0.3013\n",
      "2025-05-22 16:52:13,493 - INFO - Batch 140/146, Batch Loss: 0.2675\n",
      "2025-05-22 16:52:17,182 - INFO - Batch 146/146, Batch Loss: 0.2724\n",
      "2025-05-22 16:52:17,183 - INFO - Epoch 7/15, Train Loss: 0.2599\n",
      "2025-05-22 16:52:24,781 - INFO - Validation: Matching Dist: 0.1458, Non-Matching Dist: 0.1524, Gap: 0.0066\n",
      "2025-05-22 16:52:24,782 - INFO - Epoch 8/15, 146 batches\n",
      "2025-05-22 16:52:37,878 - INFO - Batch 20/146, Batch Loss: 0.2383\n",
      "2025-05-22 16:52:51,068 - INFO - Batch 40/146, Batch Loss: 0.3112\n",
      "2025-05-22 16:53:04,196 - INFO - Batch 60/146, Batch Loss: 0.2842\n",
      "2025-05-22 16:53:17,367 - INFO - Batch 80/146, Batch Loss: 0.2433\n",
      "2025-05-22 16:53:30,472 - INFO - Batch 100/146, Batch Loss: 0.2562\n",
      "2025-05-22 16:53:43,606 - INFO - Batch 120/146, Batch Loss: 0.2588\n",
      "2025-05-22 16:53:56,768 - INFO - Batch 140/146, Batch Loss: 0.2505\n",
      "2025-05-22 16:54:00,439 - INFO - Batch 146/146, Batch Loss: 0.2711\n",
      "2025-05-22 16:54:00,440 - INFO - Epoch 8/15, Train Loss: 0.2568\n",
      "2025-05-22 16:54:08,100 - INFO - Validation: Matching Dist: 0.1167, Non-Matching Dist: 0.1280, Gap: 0.0113\n",
      "2025-05-22 16:54:08,103 - INFO - Epoch 9/15, 146 batches\n",
      "2025-05-22 16:54:21,277 - INFO - Batch 20/146, Batch Loss: 0.2893\n",
      "2025-05-22 16:54:34,836 - INFO - Batch 40/146, Batch Loss: 0.2220\n",
      "2025-05-22 16:54:48,712 - INFO - Batch 60/146, Batch Loss: 0.2454\n",
      "2025-05-22 16:55:02,546 - INFO - Batch 80/146, Batch Loss: 0.3295\n",
      "2025-05-22 16:55:16,421 - INFO - Batch 100/146, Batch Loss: 0.2994\n",
      "2025-05-22 16:55:29,526 - INFO - Batch 120/146, Batch Loss: 0.2286\n",
      "2025-05-22 16:55:42,599 - INFO - Batch 140/146, Batch Loss: 0.2532\n",
      "2025-05-22 16:55:46,265 - INFO - Batch 146/146, Batch Loss: 0.3068\n",
      "2025-05-22 16:55:46,266 - INFO - Epoch 9/15, Train Loss: 0.2562\n",
      "2025-05-22 16:55:53,851 - INFO - Validation: Matching Dist: 0.1473, Non-Matching Dist: 0.1711, Gap: 0.0238\n",
      "2025-05-22 16:55:53,852 - INFO - Epoch 10/15, 146 batches\n",
      "2025-05-22 16:56:07,070 - INFO - Batch 20/146, Batch Loss: 0.2607\n",
      "2025-05-22 16:56:20,332 - INFO - Batch 40/146, Batch Loss: 0.2601\n",
      "2025-05-22 16:56:33,602 - INFO - Batch 60/146, Batch Loss: 0.2384\n",
      "2025-05-22 16:56:46,785 - INFO - Batch 80/146, Batch Loss: 0.2565\n",
      "2025-05-22 16:57:00,030 - INFO - Batch 100/146, Batch Loss: 0.2470\n",
      "2025-05-22 16:57:13,337 - INFO - Batch 120/146, Batch Loss: 0.2830\n",
      "2025-05-22 16:57:26,562 - INFO - Batch 140/146, Batch Loss: 0.2360\n",
      "2025-05-22 16:57:30,265 - INFO - Batch 146/146, Batch Loss: 0.2857\n",
      "2025-05-22 16:57:30,266 - INFO - Epoch 10/15, Train Loss: 0.2564\n",
      "2025-05-22 16:57:37,907 - INFO - Validation: Matching Dist: 0.1593, Non-Matching Dist: 0.2074, Gap: 0.0481\n",
      "2025-05-22 16:57:37,908 - INFO - Epoch 11/15, 146 batches\n",
      "2025-05-22 16:57:51,230 - INFO - Batch 20/146, Batch Loss: 0.2723\n",
      "2025-05-22 16:58:05,152 - INFO - Batch 40/146, Batch Loss: 0.2695\n",
      "2025-05-22 16:58:18,999 - INFO - Batch 60/146, Batch Loss: 0.2722\n",
      "2025-05-22 16:58:32,880 - INFO - Batch 80/146, Batch Loss: 0.2292\n",
      "2025-05-22 16:58:46,140 - INFO - Batch 100/146, Batch Loss: 0.2189\n",
      "2025-05-22 16:58:59,211 - INFO - Batch 120/146, Batch Loss: 0.2622\n",
      "2025-05-22 16:59:12,266 - INFO - Batch 140/146, Batch Loss: 0.2977\n",
      "2025-05-22 16:59:15,932 - INFO - Batch 146/146, Batch Loss: 0.2461\n",
      "2025-05-22 16:59:15,933 - INFO - Epoch 11/15, Train Loss: 0.2541\n",
      "2025-05-22 16:59:23,562 - INFO - Validation: Matching Dist: 0.1370, Non-Matching Dist: 0.1190, Gap: -0.0180\n",
      "2025-05-22 16:59:23,564 - INFO - Epoch 12/15, 146 batches\n",
      "2025-05-22 16:59:36,888 - INFO - Batch 20/146, Batch Loss: 0.2898\n",
      "2025-05-22 16:59:50,083 - INFO - Batch 40/146, Batch Loss: 0.2469\n",
      "2025-05-22 17:00:03,286 - INFO - Batch 60/146, Batch Loss: 0.2627\n",
      "2025-05-22 17:00:16,465 - INFO - Batch 80/146, Batch Loss: 0.2336\n",
      "2025-05-22 17:00:29,526 - INFO - Batch 100/146, Batch Loss: 0.2503\n",
      "2025-05-22 17:00:42,733 - INFO - Batch 120/146, Batch Loss: 0.2429\n",
      "2025-05-22 17:00:55,883 - INFO - Batch 140/146, Batch Loss: 0.2166\n",
      "2025-05-22 17:00:59,581 - INFO - Batch 146/146, Batch Loss: 0.2988\n",
      "2025-05-22 17:00:59,582 - INFO - Epoch 12/15, Train Loss: 0.2571\n",
      "2025-05-22 17:01:07,211 - INFO - Validation: Matching Dist: 0.1249, Non-Matching Dist: 0.1624, Gap: 0.0374\n",
      "2025-05-22 17:01:07,213 - INFO - Epoch 13/15, 146 batches\n",
      "2025-05-22 17:01:20,386 - INFO - Batch 20/146, Batch Loss: 0.2132\n",
      "2025-05-22 17:01:34,101 - INFO - Batch 40/146, Batch Loss: 0.2264\n",
      "2025-05-22 17:01:47,955 - INFO - Batch 60/146, Batch Loss: 0.2646\n",
      "2025-05-22 17:02:01,827 - INFO - Batch 80/146, Batch Loss: 0.2238\n",
      "2025-05-22 17:02:15,725 - INFO - Batch 100/146, Batch Loss: 0.2792\n",
      "2025-05-22 17:02:28,709 - INFO - Batch 120/146, Batch Loss: 0.2917\n",
      "2025-05-22 17:02:41,798 - INFO - Batch 140/146, Batch Loss: 0.2311\n",
      "2025-05-22 17:02:45,476 - INFO - Batch 146/146, Batch Loss: 0.2157\n",
      "2025-05-22 17:02:45,477 - INFO - Epoch 13/15, Train Loss: 0.2553\n",
      "2025-05-22 17:02:53,063 - INFO - Validation: Matching Dist: 0.1259, Non-Matching Dist: 0.1387, Gap: 0.0128\n",
      "2025-05-22 17:02:53,064 - INFO - Epoch 14/15, 146 batches\n",
      "2025-05-22 17:03:06,221 - INFO - Batch 20/146, Batch Loss: 0.2158\n",
      "2025-05-22 17:03:19,334 - INFO - Batch 40/146, Batch Loss: 0.2252\n",
      "2025-05-22 17:03:32,483 - INFO - Batch 60/146, Batch Loss: 0.2685\n",
      "2025-05-22 17:03:45,614 - INFO - Batch 80/146, Batch Loss: 0.2396\n",
      "2025-05-22 17:03:58,786 - INFO - Batch 100/146, Batch Loss: 0.2723\n",
      "2025-05-22 17:04:11,919 - INFO - Batch 120/146, Batch Loss: 0.3067\n",
      "2025-05-22 17:04:24,994 - INFO - Batch 140/146, Batch Loss: 0.2533\n",
      "2025-05-22 17:04:28,700 - INFO - Batch 146/146, Batch Loss: 0.2015\n",
      "2025-05-22 17:04:28,702 - INFO - Epoch 14/15, Train Loss: 0.2559\n",
      "2025-05-22 17:04:36,350 - INFO - Validation: Matching Dist: 0.1431, Non-Matching Dist: 0.1664, Gap: 0.0233\n",
      "2025-05-22 17:04:36,351 - INFO - Epoch 15/15, 146 batches\n",
      "2025-05-22 17:04:49,548 - INFO - Batch 20/146, Batch Loss: 0.2534\n",
      "2025-05-22 17:05:02,694 - INFO - Batch 40/146, Batch Loss: 0.2336\n",
      "2025-05-22 17:05:15,778 - INFO - Batch 60/146, Batch Loss: 0.2048\n",
      "2025-05-22 17:05:29,385 - INFO - Batch 80/146, Batch Loss: 0.2429\n",
      "2025-05-22 17:05:43,279 - INFO - Batch 100/146, Batch Loss: 0.2461\n",
      "2025-05-22 17:05:57,137 - INFO - Batch 120/146, Batch Loss: 0.2375\n",
      "2025-05-22 17:06:10,786 - INFO - Batch 140/146, Batch Loss: 0.2397\n",
      "2025-05-22 17:06:14,446 - INFO - Batch 146/146, Batch Loss: 0.2293\n",
      "2025-05-22 17:06:14,447 - INFO - Epoch 15/15, Train Loss: 0.2515\n",
      "2025-05-22 17:06:22,005 - INFO - Validation: Matching Dist: 0.1538, Non-Matching Dist: 0.2017, Gap: 0.0479\n",
      "2025-05-22 17:06:22,061 - INFO - Saved final model at D:\\Projects\\PhotosWorkl\\\\siamese_model_final.pth\n",
      "2025-05-22 17:06:22,209 - INFO - Saved distance plot at D:\\Projects\\PhotosWorkl\\\\distance_plot1.png\n",
      "2025-05-22 17:06:22,210 - INFO - Training completed\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7b2a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-22 16:29:37,672 - INFO - Using device: cuda\n",
      "2025-05-22 16:29:37,673 - INFO - Loading model\n",
      "2025-05-22 16:29:37,889 - INFO - Loading evaluation CSV: D:\\Projects\\PhotosWorkl\\test_pairs_balanced.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vlad\\AppData\\Local\\Temp\\ipykernel_28272\\3880791638.py:283: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-22 16:29:38,754 - INFO - Found 1166 valid pairs out of 1166\n",
      "2025-05-22 16:29:38,756 - INFO - Evaluating model on test set\n",
      "2025-05-22 16:29:46,636 - INFO - Test Set - Matching Dist: 0.2871, Non-Matching Dist: 0.3335, Gap: 0.0464\n",
      "2025-05-22 16:29:46,640 - INFO - Threshold 0.25: Accuracy 0.5858\n",
      "2025-05-22 16:29:46,644 - INFO - Threshold 0.30: Accuracy 0.5669\n",
      "2025-05-22 16:29:46,649 - INFO - Threshold 0.35: Accuracy 0.5386\n",
      "2025-05-22 16:29:46,653 - INFO - Threshold 0.40: Accuracy 0.5420\n",
      "2025-05-22 16:29:47,143 - INFO - Saved distance histogram at D:\\Projects\\PhotosWorkl\\MoreOutputs\\distance_histogram.png\n",
      "2025-05-22 16:30:27,303 - INFO - Manual test - ID: D:/Projects/PhotosWorkl/extracted_id_faces/person_706_id_face.jpg, Person: D:/Projects/finalGPT/ORIGINALS_FACE_EXTRACTIONS_128/863/863-3.jpg, Distance: 0.2531, Prediction: Matching\n",
      "2025-05-22 16:30:55,638 - INFO - Manual test - ID: D:/Projects/PhotosWorkl/extracted_id_faces/person_364_id_face.jpg, Person: D:/Projects/finalGPT/ORIGINALS_FACE_EXTRACTIONS_128/1568/1568-8.jpg, Distance: 0.2105, Prediction: Matching\n",
      "2025-05-22 16:31:08,770 - INFO - Manual test - ID: D:/Projects/PhotosWorkl/extracted_id_faces/person_395_id_face.jpg, Person: D:/Projects/finalGPT/ORIGINALS_FACE_EXTRACTIONS_128/917/917-1.jpg, Distance: 0.2577, Prediction: Matching\n",
      "2025-05-22 16:31:28,989 - INFO - Manual test - ID: D:/Projects/PhotosWorkl/extracted_id_faces/person_455_id_face.jpg, Person: D:/Projects/finalGPT/ORIGINALS_FACE_EXTRACTIONS_128/1155/1155-0.jpg, Distance: 0.2129, Prediction: Matching\n",
      "2025-05-22 16:31:36,066 - INFO - Manual test - ID: D:/Projects/PhotosWorkl/extracted_id_faces/person_455_id_face.jpg, Person: D:/Projects/finalGPT/ORIGINALS_FACE_EXTRACTIONS_128/1155/1155-0.jpg, Distance: 0.2129, Prediction: Non-Matching\n",
      "2025-05-22 16:31:49,812 - INFO - Manual test - ID: D:/Projects/PhotosWorkl/extracted_id_faces/person_435_id_face.jpg, Person: D:/Projects/finalGPT/ORIGINALS_FACE_EXTRACTIONS_128/1174/1174-0.jpg, Distance: 0.3849, Prediction: Non-Matching\n",
      "2025-05-22 16:31:57,960 - INFO - Manual test - ID: D:/Projects/PhotosWorkl/extracted_id_faces/person_435_id_face.jpg, Person: D:/Projects/finalGPT/ORIGINALS_FACE_EXTRACTIONS_128/1174/1174-0.jpg, Distance: 0.3849, Prediction: Non-Matching\n",
      "2025-05-22 16:32:00,983 - INFO - Manual test - ID: D:/Projects/PhotosWorkl/extracted_id_faces/person_435_id_face.jpg, Person: D:/Projects/finalGPT/ORIGINALS_FACE_EXTRACTIONS_128/1174/1174-0.jpg, Distance: 0.3849, Prediction: Non-Matching\n",
      "2025-05-22 16:32:04,392 - INFO - Manual test - ID: D:/Projects/PhotosWorkl/extracted_id_faces/person_435_id_face.jpg, Person: D:/Projects/finalGPT/ORIGINALS_FACE_EXTRACTIONS_128/1174/1174-0.jpg, Distance: 0.3849, Prediction: Matching\n",
      "2025-05-22 16:32:04,559 - INFO - Manual test - ID: D:/Projects/PhotosWorkl/extracted_id_faces/person_435_id_face.jpg, Person: D:/Projects/finalGPT/ORIGINALS_FACE_EXTRACTIONS_128/1174/1174-0.jpg, Distance: 0.3849, Prediction: Matching\n",
      "2025-05-22 16:32:11,882 - INFO - Manual test - ID: D:/Projects/PhotosWorkl/extracted_id_faces/person_435_id_face.jpg, Person: D:/Projects/finalGPT/ORIGINALS_FACE_EXTRACTIONS_128/1174/1174-0.jpg, Distance: 0.3849, Prediction: Non-Matching\n",
      "2025-05-22 16:32:19,437 - INFO - Manual test - ID: D:/Projects/PhotosWorkl/extracted_id_faces/person_435_id_face.jpg, Person: D:/Projects/finalGPT/ORIGINALS_FACE_EXTRACTIONS_128/1174/1174-0.jpg, Distance: 0.3849, Prediction: Matching\n",
      "2025-05-22 16:32:19,582 - INFO - Manual test - ID: D:/Projects/PhotosWorkl/extracted_id_faces/person_435_id_face.jpg, Person: D:/Projects/finalGPT/ORIGINALS_FACE_EXTRACTIONS_128/1174/1174-0.jpg, Distance: 0.3849, Prediction: Matching\n",
      "2025-05-22 16:32:27,237 - INFO - Manual test - ID: D:/Projects/PhotosWorkl/extracted_id_faces/person_226_id_face.jpg, Person: D:/Projects/finalGPT/ORIGINALS_FACE_EXTRACTIONS_128/1038/1038-1.jpg, Distance: 0.1787, Prediction: Matching\n",
      "2025-05-22 16:32:33,596 - INFO - Manual test - ID: D:/Projects/PhotosWorkl/extracted_id_faces/person_226_id_face.jpg, Person: D:/Projects/finalGPT/ORIGINALS_FACE_EXTRACTIONS_128/1038/1038-1.jpg, Distance: 0.1787, Prediction: Matching\n",
      "2025-05-22 16:32:44,574 - INFO - Manual test - ID: D:/Projects/PhotosWorkl/extracted_id_faces/person_484_id_face.jpg, Person: D:/Projects/finalGPT/ORIGINALS_FACE_EXTRACTIONS_128/720/720-0.jpg, Distance: 0.1607, Prediction: Matching\n",
      "2025-05-22 16:32:52,242 - INFO - Manual test - ID: D:/Projects/PhotosWorkl/extracted_id_faces/person_484_id_face.jpg, Person: D:/Projects/finalGPT/ORIGINALS_FACE_EXTRACTIONS_128/720/720-0.jpg, Distance: 0.1607, Prediction: Matching\n",
      "2025-05-22 16:32:52,411 - INFO - Manual test - ID: D:/Projects/PhotosWorkl/extracted_id_faces/person_484_id_face.jpg, Person: D:/Projects/finalGPT/ORIGINALS_FACE_EXTRACTIONS_128/720/720-0.jpg, Distance: 0.1607, Prediction: Matching\n",
      "2025-05-22 16:32:52,584 - INFO - Manual test - ID: D:/Projects/PhotosWorkl/extracted_id_faces/person_484_id_face.jpg, Person: D:/Projects/finalGPT/ORIGINALS_FACE_EXTRACTIONS_128/720/720-0.jpg, Distance: 0.1607, Prediction: Matching\n",
      "2025-05-22 16:32:52,775 - INFO - Manual test - ID: D:/Projects/PhotosWorkl/extracted_id_faces/person_484_id_face.jpg, Person: D:/Projects/finalGPT/ORIGINALS_FACE_EXTRACTIONS_128/720/720-0.jpg, Distance: 0.1607, Prediction: Matching\n",
      "2025-05-22 16:32:52,955 - INFO - Manual test - ID: D:/Projects/PhotosWorkl/extracted_id_faces/person_484_id_face.jpg, Person: D:/Projects/finalGPT/ORIGINALS_FACE_EXTRACTIONS_128/720/720-0.jpg, Distance: 0.1607, Prediction: Matching\n",
      "2025-05-22 16:32:53,129 - INFO - Manual test - ID: D:/Projects/PhotosWorkl/extracted_id_faces/person_484_id_face.jpg, Person: D:/Projects/finalGPT/ORIGINALS_FACE_EXTRACTIONS_128/720/720-0.jpg, Distance: 0.1607, Prediction: Matching\n",
      "2025-05-22 16:32:53,316 - INFO - Manual test - ID: D:/Projects/PhotosWorkl/extracted_id_faces/person_484_id_face.jpg, Person: D:/Projects/finalGPT/ORIGINALS_FACE_EXTRACTIONS_128/720/720-0.jpg, Distance: 0.1607, Prediction: Matching\n",
      "2025-05-22 16:32:53,498 - INFO - Manual test - ID: D:/Projects/PhotosWorkl/extracted_id_faces/person_484_id_face.jpg, Person: D:/Projects/finalGPT/ORIGINALS_FACE_EXTRACTIONS_128/720/720-0.jpg, Distance: 0.1607, Prediction: Matching\n",
      "2025-05-22 16:32:53,668 - INFO - Manual test - ID: D:/Projects/PhotosWorkl/extracted_id_faces/person_484_id_face.jpg, Person: D:/Projects/finalGPT/ORIGINALS_FACE_EXTRACTIONS_128/720/720-0.jpg, Distance: 0.1607, Prediction: Matching\n",
      "2025-05-22 16:33:04,899 - INFO - Manual test - ID: D:/Projects/PhotosWorkl/extracted_id_faces/person_192_id_face.jpg, Person: D:/Projects/finalGPT/ORIGINALS_FACE_EXTRACTIONS_128/871/871-0.jpg, Distance: 0.3391, Prediction: Matching\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, messagebox\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        in_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 128)\n",
    "        )\n",
    "    \n",
    "    def forward_one(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_one(input1)\n",
    "        output2 = self.forward_one(input2)\n",
    "        return output1, output2\n",
    "\n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        logger.info(f\"Loading evaluation CSV: {csv_file}\")\n",
    "        self.pairs_df = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.valid_indices = self._validate_paths()\n",
    "        logger.info(f\"Found {len(self.valid_indices)} valid pairs out of {len(self.pairs_df)}\")\n",
    "    \n",
    "    def _validate_paths(self):\n",
    "        valid_indices = []\n",
    "        for idx in range(len(self.pairs_df)):\n",
    "            id_img_path = self.pairs_df.iloc[idx]['id_image_path']\n",
    "            person_img_path = self.pairs_df.iloc[idx]['person_image_path']\n",
    "            try:\n",
    "                if not os.path.isfile(id_img_path) or not os.path.isfile(person_img_path):\n",
    "                    logger.warning(f\"Invalid paths at index {idx}: {id_img_path}, {person_img_path}\")\n",
    "                    continue\n",
    "                Image.open(id_img_path).convert('RGB').close()\n",
    "                Image.open(person_img_path).convert('RGB').close()\n",
    "                valid_indices.append(idx)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Cannot read images at index {idx}: {id_img_path}, {person_img_path} - {e}\")\n",
    "        return valid_indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        actual_idx = self.valid_indices[idx]\n",
    "        id_img_path = self.pairs_df.iloc[actual_idx]['id_image_path']\n",
    "        person_img_path = self.pairs_df.iloc[actual_idx]['person_image_path']\n",
    "        label = self.pairs_df.iloc[actual_idx]['label']\n",
    "        \n",
    "        try:\n",
    "            id_img = Image.open(id_img_path).convert('RGB')\n",
    "            person_img = Image.open(person_img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading images at index {actual_idx}: {id_img_path}, {person_img_path} - {e}\")\n",
    "            return None\n",
    "        \n",
    "        if self.transform:\n",
    "            try:\n",
    "                id_img = self.transform(id_img)\n",
    "                person_img = self.transform(person_img)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error applying transform at index {actual_idx}: {e}\")\n",
    "                return None\n",
    "        \n",
    "        return id_img, person_img, torch.tensor(label, dtype=torch.float32), id_img_path, person_img_path\n",
    "\n",
    "def evaluate_model(model, test_loader, device, output_dir, thresholds=[0.25, 0.3, 0.35, 0.4]):\n",
    "    logger.info(\"Evaluating model on test set\")\n",
    "    model.eval()\n",
    "    matching_distances = []\n",
    "    non_matching_distances = []\n",
    "    distances = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            if batch is None:\n",
    "                continue\n",
    "            img1, img2, label, _, _ = batch\n",
    "            img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "            output1, output2 = model(img1, img2)\n",
    "            dist = torch.nn.functional.pairwise_distance(output1, output2)\n",
    "            distances.extend(dist.cpu().numpy())\n",
    "            labels.extend(label.cpu().numpy())\n",
    "            for d, l in zip(dist.cpu().numpy(), label.cpu().numpy()):\n",
    "                if l == 1:\n",
    "                    matching_distances.append(d)\n",
    "                else:\n",
    "                    non_matching_distances.append(d)\n",
    "    \n",
    "    mean_matching = np.mean(matching_distances) if matching_distances else float('inf')\n",
    "    mean_non_matching = np.mean(non_matching_distances) if non_matching_distances else 0.0\n",
    "    gap = mean_non_matching - mean_matching\n",
    "    logger.info(f\"Test Set - Matching Dist: {mean_matching:.4f}, Non-Matching Dist: {mean_non_matching:.4f}, Gap: {gap:.4f}\")\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        predictions = [1 if d < threshold else 0 for d in distances]\n",
    "        accuracy = np.mean([1 if p == l else 0 for p, l in zip(predictions, labels)])\n",
    "        logger.info(f\"Threshold {threshold:.2f}: Accuracy {accuracy:.4f}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(matching_distances, bins=50, alpha=0.5, label='Matching', color='blue')\n",
    "    plt.hist(non_matching_distances, bins=50, alpha=0.5, label='Non-Matching', color='red')\n",
    "    plt.xlabel('Distance')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distance Histogram: Matching vs Non-Matching Pairs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plot_path = os.path.join(output_dir, 'distance_histogram.png')\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    logger.info(f\"Saved distance histogram at {plot_path}\")\n",
    "    \n",
    "    return mean_matching, mean_non_matching, gap, distances, labels\n",
    "\n",
    "class SiameseGUI:\n",
    "    def __init__(self, root, model, transform, device):\n",
    "        self.root = root\n",
    "        self.model = model\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "        self.root.title(\"Siamese Network Manual Testing\")\n",
    "        \n",
    "        tk.Label(root, text=\"ID Image Path:\").grid(row=0, column=0, padx=5, pady=5)\n",
    "        self.id_path_entry = tk.Entry(root, width=50)\n",
    "        self.id_path_entry.grid(row=0, column=1, padx=5, pady=5)\n",
    "        \n",
    "        tk.Label(root, text=\"Person Image Path:\").grid(row=1, column=0, padx=5, pady=5)\n",
    "        self.person_path_entry = tk.Entry(root, width=50)\n",
    "        self.person_path_entry.grid(row=1, column=1, padx=5, pady=5)\n",
    "        \n",
    "        tk.Label(root, text=\"True Label (0/1, optional):\").grid(row=2, column=0, padx=5, pady=5)\n",
    "        self.label_entry = tk.Entry(root, width=10)\n",
    "        self.label_entry.grid(row=2, column=1, padx=5, pady=5, sticky='w')\n",
    "        \n",
    "        tk.Label(root, text=\"Threshold:\").grid(row=3, column=0, padx=5, pady=5)\n",
    "        self.threshold_entry = tk.Entry(root, width=10)\n",
    "        self.threshold_entry.grid(row=3, column=1, padx=5, pady=5, sticky='w')\n",
    "        self.threshold_entry.insert(0, \"0.3\")\n",
    "        \n",
    "        self.id_image_label = tk.Label(root)\n",
    "        self.id_image_label.grid(row=4, column=0, padx=5, pady=5)\n",
    "        self.person_image_label = tk.Label(root)\n",
    "        self.person_image_label.grid(row=4, column=1, padx=5, pady=5)\n",
    "        \n",
    "        self.result_label = tk.Label(root, text=\"\", font=(\"Arial\", 12))\n",
    "        self.result_label.grid(row=5, column=0, columnspan=2, padx=5, pady=5)\n",
    "        \n",
    "        tk.Button(root, text=\"Test Pair\", command=self.test_pair).grid(row=6, column=0, columnspan=2, pady=10)\n",
    "        tk.Button(root, text=\"Clear\", command=self.clear).grid(row=7, column=0, columnspan=2, pady=5)\n",
    "    \n",
    "    def test_pair(self):\n",
    "        id_path = self.id_path_entry.get().strip()\n",
    "        person_path = self.person_path_entry.get().strip()\n",
    "        threshold = self.threshold_entry.get().strip()\n",
    "        label = self.label_entry.get().strip()\n",
    "        \n",
    "        try:\n",
    "            threshold = float(threshold)\n",
    "            if threshold <= 0:\n",
    "                raise ValueError(\"Threshold must be positive\")\n",
    "        except ValueError:\n",
    "            messagebox.showerror(\"Error\", \"Invalid threshold. Enter a positive number (e.g., 0.3).\")\n",
    "            return\n",
    "        \n",
    "        if not os.path.isfile(id_path) or not os.path.isfile(person_path):\n",
    "            messagebox.showerror(\"Error\", \"Invalid image paths. Check files exist.\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            id_img = Image.open(id_path).convert('RGB')\n",
    "            person_img = Image.open(person_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Cannot load images: {e}\")\n",
    "            return\n",
    "        \n",
    "        id_img_tk = id_img.resize((224, 224))\n",
    "        person_img_tk = person_img.resize((224, 224))\n",
    "        self.id_photo = ImageTk.PhotoImage(id_img_tk)\n",
    "        self.person_photo = ImageTk.PhotoImage(person_img_tk)\n",
    "        self.id_image_label.config(image=self.id_photo)\n",
    "        self.person_image_label.config(image=self.person_photo)\n",
    "        \n",
    "        try:\n",
    "            id_tensor = self.transform(id_img).unsqueeze(0).to(self.device)\n",
    "            person_tensor = self.transform(person_img).unsqueeze(0).to(self.device)\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                output1, output2 = self.model(id_tensor, person_tensor)\n",
    "                distance = torch.nn.functional.pairwise_distance(output1, output2).item()\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Error computing distance: {e}\")\n",
    "            return\n",
    "        \n",
    "        prediction = \"Matching\" if distance < threshold else \"Non-Matching\"\n",
    "        result_text = f\"Distance: {distance:.4f}\\nPrediction: {prediction} (Threshold: {threshold:.2f})\"\n",
    "        if label:\n",
    "            try:\n",
    "                label = int(label)\n",
    "                if label in [0, 1]:\n",
    "                    result_text += f\"\\nTrue Label: {'Matching' if label == 1 else 'Non-Matching'}\"\n",
    "                    result_text += f\"\\nCorrect: {((distance < threshold) == (label == 1))}\"\n",
    "            except ValueError:\n",
    "                result_text += \"\\nInvalid true label (use 0 or 1)\"\n",
    "        \n",
    "        self.result_label.config(text=result_text)\n",
    "        logger.info(f\"Manual test - ID: {id_path}, Person: {person_path}, Distance: {distance:.4f}, Prediction: {prediction}\")\n",
    "    \n",
    "    def clear(self):\n",
    "        self.id_path_entry.delete(0, tk.END)\n",
    "        self.person_path_entry.delete(0, tk.END)\n",
    "        self.label_entry.delete(0, tk.END)\n",
    "        self.threshold_entry.delete(0, tk.END)\n",
    "        self.threshold_entry.insert(0, \"0.3\")\n",
    "        self.id_image_label.config(image='')\n",
    "        self.person_image_label.config(image='')\n",
    "        self.result_label.config(text='')\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    model_path = r\"D:\\Projects\\PhotosWorkl\\siamese_model_final.pth\"\n",
    "    test_csv = r\"D:\\Projects\\PhotosWorkl\\test_pairs_balanced.csv\"\n",
    "    output_dir = r\"D:\\Projects\\PhotosWorkl\\MoreOutputs\"\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    # Load model\n",
    "    logger.info(\"Loading model\")\n",
    "    model = SiameseNetwork().to(device)\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_dataset = SiameseDataset(test_csv, transform=transform)\n",
    "    if len(test_dataset) == 0:\n",
    "        logger.error(\"No valid test pairs. Check CSV paths and image accessibility.\")\n",
    "        return\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    mean_matching, mean_non_matching, gap, distances, labels = evaluate_model(\n",
    "        model, test_loader, device, output_dir, thresholds=[0.25, 0.3, 0.35, 0.4]\n",
    "    )\n",
    "    \n",
    "    root = tk.Tk()\n",
    "    app = SiameseGUI(root, model, transform, device)\n",
    "    root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from PIL import ImageTk\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
