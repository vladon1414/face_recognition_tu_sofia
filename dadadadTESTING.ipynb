{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fb4b3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vlad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Vlad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\facenet_pytorch\\models\\mtcnn.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "C:\\Users\\Vlad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\facenet_pytorch\\models\\mtcnn.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "C:\\Users\\Vlad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\facenet_pytorch\\models\\mtcnn.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Siamese Network definition (matches your trained model)\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.cnn = InceptionResnetV1(pretrained=\"vggface2\").eval()\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        return self.cnn(x)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        emb1 = self.forward_one(x1)\n",
    "        emb2 = self.forward_one(x2)\n",
    "        return emb1, emb2\n",
    "\n",
    "# Initialize MTCNN\n",
    "mtcnn = MTCNN(keep_all=False, device=device)\n",
    "\n",
    "# Face extraction function\n",
    "def extract_face(image_path, target_size=(160, 160), margin=20):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Failed to load {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert to RGB for MTCNN\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    boxes, _ = mtcnn.detect(img_rgb)\n",
    "    \n",
    "    if boxes is not None and len(boxes) > 0:\n",
    "        x1, y1, x2, y2 = map(int, boxes[0])\n",
    "        # Add margin and clamp to image bounds\n",
    "        x1 = max(0, x1 - margin)\n",
    "        y1 = max(0, y1 - margin)\n",
    "        x2 = min(img.shape[1], x2 + margin)\n",
    "        y2 = min(img.shape[0], y2 + margin)\n",
    "        \n",
    "        # Check for valid crop\n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            print(f\"Invalid crop coordinates for {image_path}: ({x1}, {y1}, {x2}, {y2})\")\n",
    "            return None\n",
    "        \n",
    "        # Crop face (BGR format)\n",
    "        face = img[y1:y2, x1:x2]\n",
    "        # Convert to RGB PIL Image\n",
    "        face_rgb = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "        face_pil = Image.fromarray(face_rgb)\n",
    "        \n",
    "        # Resize with aspect ratio preservation\n",
    "        old_size = face_pil.size  # (width, height)\n",
    "        ratio = min(target_size[0] / old_size[0], target_size[1] / old_size[1])\n",
    "        new_size = (int(old_size[0] * ratio), int(old_size[1] * ratio))\n",
    "        face_pil = face_pil.resize(new_size, Image.LANCZOS)\n",
    "        # Pad to target size\n",
    "        new_image = Image.new(\"RGB\", target_size, (128, 128, 128))  # Gray padding\n",
    "        new_image.paste(face_pil, ((target_size[0] - new_size[0]) // 2, (target_size[1] - new_size[1]) // 2))\n",
    "        \n",
    "        return new_image\n",
    "    else:\n",
    "        print(f\"No face detected in {image_path}\")\n",
    "        return None\n",
    "\n",
    "# Preprocessing transform for model input\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# load images from path not  extracting with mtcnn \n",
    "def load_cropped_face(image_path, target_size=(160, 160)):\n",
    "    try:\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        img = img.resize(target_size, Image.LANCZOS)\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Manual testing function\n",
    "def test_manual_pair(model, id_image_path, selfie_image_path, transform):\n",
    "    model.eval()\n",
    "    \n",
    "    #uses mtcnn to first extarct a face\n",
    "    id_img = extract_face(id_image_path)\n",
    "    selfie_img = extract_face(selfie_image_path)\n",
    "    \n",
    "    # loads extracted images\n",
    "    # id_img = load_cropped_face(id_image_path)\n",
    "    # selfie_img = load_cropped_face(selfie_image_path)\n",
    "    # Check if extraction succeeded\n",
    "    if id_img is None or selfie_img is None:\n",
    "        print(\"Face extraction failed for one or both images. Aborting.\")\n",
    "        return\n",
    "    \n",
    "    # # Save raw faces for debugging\n",
    "    # id_img.save(\"debug_id_extracted.png\")\n",
    "    # selfie_img.save(\"debug_selfie_extracted.png\")\n",
    "    # print(\"Extracted faces saved as 'debug_id_extracted.png' and 'debug_selfie_extracted.png'\")\n",
    "    \n",
    "    # Prepare display images (grayscale)\n",
    "    id_img_display = id_img.convert(\"L\")\n",
    "    selfie_img_display = selfie_img.convert(\"L\")\n",
    "    \n",
    "    # Prepare model input\n",
    "    id_tensor = transform(id_img).unsqueeze(0).to(device)\n",
    "    selfie_tensor = transform(selfie_img).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Debug shapes\n",
    "    print(f\"ID tensor shape: {id_tensor.shape}\")\n",
    "    print(f\"Selfie tensor shape: {selfie_tensor.shape}\")\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        emb1, emb2 = model(id_tensor, selfie_tensor)\n",
    "    \n",
    "    # Compute distance and similarity\n",
    "    dist = torch.sqrt(torch.sum((emb1 - emb2) ** 2) + 1e-10).item()\n",
    "    similarity = max(0, 100 * (1 - dist / 1.5))  # Heuristic, tune if needed\n",
    "    \n",
    "    # Display images and similarity\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax1.imshow(id_img_display, cmap=\"gray\")\n",
    "    ax1.set_title(\"ID Image\")\n",
    "    ax1.axis(\"off\")\n",
    "    ax2.imshow(selfie_img_display, cmap=\"gray\")\n",
    "    ax2.set_title(\"Selfie Image\")\n",
    "    ax2.axis(\"off\")\n",
    "    plt.suptitle(f\"Similarity: {similarity:.2f}%\", fontsize=16)\n",
    "    # plt.savefig(\"similarity_plot.png\")\n",
    "    # print(\"Plot saved as 'similarity_plot.png'\")\n",
    "    plt.show(block=True)\n",
    "    \n",
    "    print(f\"Distance: {dist:.4f}, Similarity: {similarity:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca2e7a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vlad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\facenet_pytorch\\models\\inception_resnet_v1.py:329: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(cached_file)\n",
      "C:\\Users\\Vlad\\AppData\\Local\\Temp\\ipykernel_25736\\1234394270.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"siamese_facenet.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID tensor shape: torch.Size([1, 3, 160, 160])\n",
      "Selfie tensor shape: torch.Size([1, 3, 160, 160])\n",
      "Distance: 1.7323, Similarity: 0.00%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load trained model\n",
    "    model = SiameseNetwork().to(device)\n",
    "    model.load_state_dict(torch.load(\"siamese_facenet.pth\"))\n",
    "    model.eval()\n",
    "\n",
    "    # Test pair\n",
    "    test_id_image = r\"D:/Projects/PhotosWorkl/extracted_id_faces/person_116_id_face.jpg\"\n",
    "    test_selfie_image = r\"D:/Projects/finalGPT/originals/1191/1191-5.jpg\"\n",
    "    \n",
    "\n",
    "    # Test with face extraction\n",
    "    test_manual_pair(model, test_id_image, test_selfie_image, transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7085549e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def evaluate_siamese_on_csv(model, csv_path, transform, threshold=1.0, save_path=None):\n",
    "    \"\"\"\n",
    "    Evaluate Siamese model on a dataset of pairs (CSV format).\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Siamese model (nn.Module).\n",
    "        csv_path: Path to CSV file with [id_path, selfie_path, label].\n",
    "        transform: Transform to preprocess input images.\n",
    "        threshold: Distance threshold for deciding match (default 1.0).\n",
    "        save_path: If not None, saves confusion matrix to this path.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if list(df.columns)[:3] == [0, 1, 2]:  # no header\n",
    "        df = pd.read_csv(csv_path, header=None, names=[\"id_path\", \"selfie_path\", \"label\"])\n",
    "    else:\n",
    "        # Normalize expected names\n",
    "        col_map = {c.lower(): c for c in df.columns}\n",
    "        df = df.rename(columns={\n",
    "            col_map.get(\"id_path\", list(df.columns)[0]): \"id_path\",\n",
    "            col_map.get(\"selfie_path\", list(df.columns)[1]): \"selfie_path\",\n",
    "            col_map.get(\"label\", list(df.columns)[2]): \"label\"\n",
    "        })\n",
    "    \n",
    "    all_labels, all_preds = [], []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        id_path = row[\"id_path\"]\n",
    "        selfie_path = row[\"selfie_path\"]\n",
    "        label = int(row[\"label\"])\n",
    "        \n",
    "        id_img = load_cropped_face(id_path)   # use your load function (no mtcnn)\n",
    "        selfie_img = load_cropped_face(selfie_path)\n",
    "        \n",
    "        if id_img is None or selfie_img is None:\n",
    "            print(f\"Skipping {id_path}, {selfie_path}\")\n",
    "            continue\n",
    "        \n",
    "        id_tensor = transform(id_img).unsqueeze(0).to(device)\n",
    "        selfie_tensor = transform(selfie_img).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            emb1, emb2 = model(id_tensor, selfie_tensor)\n",
    "            dist = torch.sqrt(torch.sum((emb1 - emb2) ** 2) + 1e-10).item()\n",
    "            pred = 1 if dist < threshold else 0\n",
    "        \n",
    "        all_labels.append(label)\n",
    "        all_preds.append(pred)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = np.zeros((2, 2), dtype=int)\n",
    "    for t, p in zip(all_labels, all_preds):\n",
    "        cm[t, p] += 1\n",
    "    \n",
    "    accuracy = (cm[0, 0] + cm[1, 1]) / np.sum(cm)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, cmap=\"Blues\")\n",
    "    \n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_xticklabels([\"Pred 0 (Different)\", \"Pred 1 (Same)\"])\n",
    "    ax.set_yticklabels([\"True 0 (Different)\", \"True 1 (Same)\"])\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"red\")\n",
    "    \n",
    "    plt.title(f\"Confusion Matrix - Siamese (Threshold={threshold})\")\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "        print(f\"Confusion matrix saved to {save_path}\")\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e815c624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[1569  760]\n",
      " [ 831 1498]]\n",
      "Accuracy: 65.84%\n",
      "Confusion matrix saved to siamese_confusion_matrix_Pretraind_OWN_DAT_ONDEVELOPMENTDATA.png\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with your Siamese network on test pairs\n",
    "evaluate_siamese_on_csv(\n",
    "    model, \n",
    "    \"D:/Projects/PhotosWorkl/train_pairs_balanced.csv\", \n",
    "    transform=transform,\n",
    "    threshold=1.0, \n",
    "    save_path=\"siamese_confusion_matrix_Pretraind_OWN_DAT_ONDEVELOPMENTDATA.png\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
